<div align="center">

<img src="https://github.com/monologg/KoBigBird/raw/master/.github/images/kobigbird-logo.png" width="400px">

<h1>Pretrained BigBird Model for Korean</h1>

<p align="center">
  <a href="#what-is-bigbird">What is BigBird</a> â€¢
  <a href="#how-to-use">How to Use</a> â€¢
  <a href="#pretraining">Pretraining</a> â€¢
  <a href="#evaluation-result">Evaluation Result</a> â€¢
  <a href="#docs">Docs</a> â€¢
  <a href="#citation">Citation</a>
</p>

<p>
    <b>í•œêµ­ì–´</b> |
    <a href="README_EN.md">English</a>
</p>

<p align="center">
    <a href="https://github.com/monologg/KoBigBird/blob/master/LICENSE">
        <img alt="Apache 2.0" src="https://img.shields.io/badge/license-Apache%202.0-yellow.svg">
    </a>
    <a href="https://github.com/monologg/KoBigBird/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/monologg/KoBigBird">
    </a>
    <a href="https://github.com/monologg/KoBigBird/actions/workflows/linter.yml">
        <img alt="linter" src="https://github.com/monologg/KoBigBird/actions/workflows/linter.yml/badge.svg">
    </a>
    <a href="https://doi.org/10.5281/zenodo.5654154">
        <img alt="DOI" src="https://img.shields.io/badge/DOI-10.5281%2Fzenodo.5654154-blue">
    </a>
</p>

</div>

## What is BigBird?

<img width="700px" src="https://github.com/monologg/KoBigBird/raw/master/.github/images/sparse-attention.png">

[BigBird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)ì—ì„œ ì†Œê°œëœ **sparse-attention** ê¸°ë°˜ì˜ ëª¨ë¸ë¡œ, ì¼ë°˜ì ì¸ BERTë³´ë‹¤ **ë” ê¸´ sequence**ë¥¼ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ¦… **Longer Sequence** - ìµœëŒ€ 512ê°œì˜ tokenì„ ë‹¤ë£° ìˆ˜ ìˆëŠ” BERTì˜ 8ë°°ì¸ **ìµœëŒ€ 4096ê°œì˜ token**ì„ ë‹¤ë£¸

â±ï¸ **Computational Efficiency** - Full attentionì´ ì•„ë‹Œ **Sparse Attention**ì„ ì´ìš©í•˜ì—¬ O(n<sup>2</sup>)ì—ì„œ <b>O(n)</b>ìœ¼ë¡œ ê°œì„ 

## How to Use

- ğŸ¤— [Huggingface Hub](https://huggingface.co/monologg/kobigbird-bert-base)ì— ì—…ë¡œë“œëœ ëª¨ë¸ì„ ê³§ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:)
- ì¼ë¶€ ì´ìŠˆê°€ í•´ê²°ëœ `transformers>=4.11.0` ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ([MRC ì´ìŠˆ ê´€ë ¨ PR](https://github.com/huggingface/transformers/pull/13143))
- **BigBirdTokenizer ëŒ€ì‹ ì— `BertTokenizer` ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. (`AutoTokenizer` ì‚¬ìš©ì‹œ `BertTokenizer`ê°€ ë¡œë“œë©ë‹ˆë‹¤.)**
- ìì„¸í•œ ì‚¬ìš©ë²•ì€ [BigBird Tranformers documentation](https://huggingface.co/transformers/model_doc/bigbird.html)ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

```python
from transformers import AutoModel, AutoTokenizer

model = AutoModel.from_pretrained("monologg/kobigbird-bert-base")  # BigBirdModel
tokenizer = AutoTokenizer.from_pretrained("monologg/kobigbird-bert-base")  # BertTokenizer
```

## Pretraining

> ìì„¸í•œ ë‚´ìš©ì€ [[Pretraining BigBird]](pretrain/README.md) ì°¸ê³ 

|                         | Hardware | Max len |   LR | Batch | Train Step | Warmup Step |
| :---------------------- | -------: | ------: | ---: | ----: | ---------: | ----------: |
| **KoBigBird-BERT-Base** | TPU v3-8 |    4096 | 1e-4 |    32 |         2M |         20k |

- ëª¨ë‘ì˜ ë§ë­‰ì¹˜, í•œêµ­ì–´ ìœ„í‚¤, Common Crawl, ë‰´ìŠ¤ ë°ì´í„° ë“± ë‹¤ì–‘í•œ ë°ì´í„°ë¡œ í•™ìŠµ
- `ITC (Internal Transformer Construction)` ëª¨ë¸ë¡œ í•™ìŠµ ([ITC vs ETC](https://huggingface.co/blog/big-bird#itc-vs-etc))

## Evaluation Result

### 1. Short Sequence (<=512)

> ìì„¸í•œ ë‚´ìš©ì€ [[Finetune on Short Sequence Dataset]](docs/short_seq_evaluation_ko.md) ì°¸ê³ 

|                         | NSMC<br>(acc) | KLUE-NLI<br>(acc) | KLUE-STS<br>(pearsonr) | Korquad 1.0<br>(em/f1) | KLUE MRC<br>(em/rouge-w) |
| :---------------------- | :-----------: | :---------------: | :--------------------: | :--------------------: | :----------------------: |
| KoELECTRA-Base-v3       |     91.13     |       86.87       |       **93.14**        |     85.66 / 93.94      |      59.54 / 65.64       |
| KLUE-RoBERTa-Base       |     91.16     |       86.30       |         92.91          |     85.35 / 94.53      |      69.56 / 74.64       |
| **KoBigBird-BERT-Base** |   **91.18**   |     **87.17**     |         92.61          |   **87.08 / 94.71**    |    **70.33 / 75.34**     |

### 2. Long Sequence (>=1024)

> ìì„¸í•œ ë‚´ìš©ì€ [[Finetune on Long Sequence Dataset]](finetune/README.md) ì°¸ê³ 

|                         | TyDi QA<br/>(em/f1) | Korquad 2.1<br/>(em/f1) | Fake News<br/>(f1) | Modu Sentiment<br/>(f1-macro) |
| :---------------------- | :-----------------: | :---------------------: | :----------------: | :---------------------------: |
| KLUE-RoBERTa-Base       |    76.80 / 78.58    |      55.44 / 73.02      |       95.20        |             42.61             |
| **KoBigBird-BERT-Base** |  **79.13 / 81.30**  |    **67.77 / 82.03**    |     **98.85**      |           **45.42**           |

## Docs

- [Pretraing BigBird](pretrain/README.md)
- [Finetune on Short Sequence Dataset](docs/short_seq_evaluation_ko.md)
- [Finetune on Long Sequence Dataset](finetune/README.md)
- [Download Tensorflow v1 checkpoint](docs/download_tfv1_ckpt.md)
- [GPU Benchmark result](docs/gpu_benchmark.md)

## Citation

KoBigBirdë¥¼ ì‚¬ìš©í•˜ì‹ ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì¸ìš©í•´ì£¼ì„¸ìš”.

```bibtex
@software{jangwon_park_2021_5654154,
  author       = {Jangwon Park and Donggyu Kim},
  title        = {KoBigBird: Pretrained BigBird Model for Korean},
  month        = nov,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0.0},
  doi          = {10.5281/zenodo.5654154},
  url          = {https://doi.org/10.5281/zenodo.5654154}
}
```

## Contributors

[Jangwon Park](https://github.com/monologg) and [Donggyu Kim](https://github.com/donggyukimc)

## Acknowledgements

KoBigBirdëŠ” Tensorflow Research Cloud (TFRC) í”„ë¡œê·¸ë¨ì˜ Cloud TPU ì§€ì›ìœ¼ë¡œ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.

ë˜í•œ ë©‹ì§„ ë¡œê³ ë¥¼ ì œê³µí•´ì£¼ì‹  [Seyun Ahn](https://www.instagram.com/ahnsy13)ë‹˜ê»˜ ê°ì‚¬ë¥¼ ì „í•©ë‹ˆë‹¤.
